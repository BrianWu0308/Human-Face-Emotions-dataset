{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5736b8b0-92c6-4f29-b2bc-fb8701747da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62ab0d8-77ce-420d-a7d6-06b5858a894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: brian3698741 (brian3698741-national-tsing-hua-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"9fdbffa3fe214bfa40e7ffaa6487c4c80e5a5b4f\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe974834-bc6c-4d46-8496-985fee2a8887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.7s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Desktop\\Pytorch, HF exercise\\Pytorch\\wandb\\run-20251119_022001-bryozm6l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brian3698741-national-tsing-hua-university/Human%20Face%20Emotions%20dataset/runs/bryozm6l' target=\"_blank\">baseline</a></strong> to <a href='https://wandb.ai/brian3698741-national-tsing-hua-university/Human%20Face%20Emotions%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brian3698741-national-tsing-hua-university/Human%20Face%20Emotions%20dataset' target=\"_blank\">https://wandb.ai/brian3698741-national-tsing-hua-university/Human%20Face%20Emotions%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brian3698741-national-tsing-hua-university/Human%20Face%20Emotions%20dataset/runs/bryozm6l' target=\"_blank\">https://wandb.ai/brian3698741-national-tsing-hua-university/Human%20Face%20Emotions%20dataset/runs/bryozm6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Human Face Emotions dataset\", name=\"baseline\", config={\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 5e-2, \n",
    "    \"num_workers\": 0, \n",
    "})\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49494913-62dd-4981-96a6-3f6cbcb1ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "ROOT = '.\\Datasets\\Human Face Emotions'\n",
    "classes = sorted(os.listdir(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0883d6e-1ab4-4ddf-a969-21d11c0a0548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Angry', 'Fear', 'Happy', 'Sad', 'Suprise']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778f4296-2ddc-4e61-9dde-ad3eda076381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用glob抓資料夾底下所有同類型的檔案: ROOT\\cls\\*.png\n",
    "samples = []\n",
    "for cls in classes:\n",
    "    for img_path in glob(os.path.join(ROOT, cls, '*')):\n",
    "        samples.append((img_path, cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a31759-b12e-4857-9250-62ce46082d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('.\\\\Datasets\\\\Human Face Emotions\\\\Angry\\\\0.png', 'Angry'),\n",
       "  ('.\\\\Datasets\\\\Human Face Emotions\\\\Angry\\\\1.png', 'Angry'),\n",
       "  ('.\\\\Datasets\\\\Human Face Emotions\\\\Angry\\\\10.png', 'Angry'),\n",
       "  ('.\\\\Datasets\\\\Human Face Emotions\\\\Angry\\\\10002.png', 'Angry'),\n",
       "  ('.\\\\Datasets\\\\Human Face Emotions\\\\Angry\\\\10005903.png', 'Angry')],\n",
       " 59099)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[:5], len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0951576-3db2-4826-a307-4ebff59dedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# [「要放的東西」 for 「變數」 in 「可迭代物」]\n",
    "labels = [s[1] for s in samples]\n",
    "tr_samples, va_samples = train_test_split(\n",
    "    samples, \n",
    "    test_size=0.2, \n",
    "    shuffle=True, \n",
    "    # stratify = labels: 讓training, validation set中的labels可以被等比例切分\n",
    "    stratify=labels, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84f0e92-e234-4b2d-8f01-619306da4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 47279\n",
      "Validation size: 11820\n"
     ]
    }
   ],
   "source": [
    "print(f'Training size: {len(tr_samples)}')\n",
    "print(f'Validation size: {len(va_samples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2bf764-b900-4c09-bf02-231d5b8600a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training set: \n",
      "    Angry: 17%\n",
      "    Fear: 16%\n",
      "    Happy: 31%\n",
      "    Sad: 21%\n",
      "    Suprise: 14%\n",
      "\n",
      "In validation set: \n",
      "    Angry: 17%\n",
      "    Fear: 16%\n",
      "    Happy: 31%\n",
      "    Sad: 21%\n",
      "    Suprise: 14%\n"
     ]
    }
   ],
   "source": [
    "print('In training set: ')\n",
    "for cls in classes:\n",
    "    print(f'    {cls}: {([s[1] for s in tr_samples].count(cls) / len(tr_samples) * 100):.0f}%')\n",
    "print('\\nIn validation set: ')\n",
    "for cls in classes:\n",
    "    print(f'    {cls}: {([s[1] for s in va_samples].count(cls) / len(va_samples) * 100):.0f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "925e829b-ea99-4fe2-95d1-aedd809aba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None, target_transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.samples[idx][0]).convert('L')\n",
    "        label = self.samples[idx][1]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return img, label\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ece669-124c-43d9-b1dd-b045fe275cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Image transform = Preprocess(ex: resize to 224x224) + Data augmentation + To Tensor + Normalize\n",
    "tr_tf = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "va_tf = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c201a65c-0cea-4c65-b869-a93c37c62479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 48]) 2\n"
     ]
    }
   ],
   "source": [
    "# 建label_dict讓string label可以map到一個idx\n",
    "label_dict = {}\n",
    "for idx, cls in enumerate(classes):\n",
    "    label_dict[cls] = idx\n",
    "def label_to_idx(label):\n",
    "    return label_dict[label]\n",
    "\n",
    "tr_ds = ImageDataset(tr_samples, transform=tr_tf, target_transform=label_to_idx)\n",
    "va_ds = ImageDataset(va_samples, transform=va_tf, target_transform=label_to_idx)\n",
    "\n",
    "X0, y0 = tr_ds[1]\n",
    "print(X0.shape, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6f75e3b-c2f7-4a1c-9293-9fb1a6cd4fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 0\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 5e-2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "tr_dl = DataLoader(tr_ds, batch_size=config.batch_size, shuffle=True, num_workers=config.num_workers)\n",
    "va_dl = DataLoader(va_ds, batch_size=config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "794df019-66dc-4a50-8256-1fa5bb68b176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches of training set: 1478\n",
      "Total batches of validation set: 370\n"
     ]
    }
   ],
   "source": [
    "print(f'Total batches of training set: {len(tr_dl)}')\n",
    "print(f'Total batches of validation set: {len(va_dl)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dabb005-ba87-4eb8-869d-2cb25f90fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_conv1x1=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.path1 = nn.Sequential(\n",
    "            # 第一個conv2d可以用strides來降低H, W(ex: s=2 H, W / = 2)\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=strides, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(), \n",
    "            # 第二個conv2d必須保持H, W不變 所以s=1\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        # 若有需要則在shortcut加上1x1conv來調整channel數或H, W大小(用strides調整)\n",
    "        if use_conv1x1 or (in_channels != out_channels or strides != 1):\n",
    "            self.path2 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=strides, padding=0)\n",
    "        else:\n",
    "            self.path2 = nn.Identity()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.path1(X)\n",
    "        shortcut = self.path2(X)\n",
    "        # 此處的relu只是一個function而非layer\n",
    "        return F.relu(out + shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d14a64b-43e2-427c-aec2-dd7d82625d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, in_channels, out_num):\n",
    "        super().__init__()\n",
    "        self.stage0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3), \n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1), \n",
    "        )\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ResBlock(64, 64, strides=1),\n",
    "            ResBlock(64, 64, strides=1),\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResBlock(64, 128, strides=2),\n",
    "            ResBlock(128, 128, strides=1),\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResBlock(128, 256, strides=2),\n",
    "            ResBlock(256, 256, strides=1),\n",
    "        )\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResBlock(256, 512, strides=2),\n",
    "            ResBlock(512, 512, strides=1),\n",
    "        )\n",
    "        self.stage5 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512, out_num)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        X = self.stage0(X)\n",
    "        X = self.stage1(X)\n",
    "        X = self.stage2(X)\n",
    "        X = self.stage3(X)\n",
    "        X = self.stage4(X)\n",
    "        X = self.stage5(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09dbbaef-59e1-402c-947d-a592b5981e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewResNet18(nn.Module):\n",
    "    def __init__(self, in_channels, out_num):\n",
    "        super().__init__()\n",
    "        self.stage0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1), \n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(), \n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        )\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ResBlock(64, 64, strides=1),\n",
    "            ResBlock(64, 64, strides=1),\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResBlock(64, 128, strides=2),\n",
    "            ResBlock(128, 128, strides=1),\n",
    "        )\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResBlock(128, 256, strides=2),\n",
    "            ResBlock(256, 256, strides=1),\n",
    "        )\n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResBlock(256, 512, strides=2),\n",
    "            ResBlock(512, 512, strides=1),\n",
    "        )\n",
    "        self.stage5 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512, out_num)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        X = self.stage0(X)\n",
    "        X = self.stage1(X)\n",
    "        X = self.stage2(X)\n",
    "        X = self.stage3(X)\n",
    "        X = self.stage4(X)\n",
    "        X = self.stage5(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fd1a0cd-0fe2-4df4-9fcb-045ea71b0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NewResNet18(1, 5)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c4deda-8ba0-45ba-a522-f1fed8fbe250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|███████████████████████████████████████████████████████████████████| 1478/1478 [06:34<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss = 1.2598\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m tr_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tr_dl)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Training loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mavg_loss\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: tr_acc})\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_loss' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    for Xb, yb in tqdm(tr_dl, desc=f\"Epoch {epoch+1}/{config.epochs}\"):\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        preds = model(Xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    tr_loss /= len(tr_dl)\n",
    "    print(f\"Epoch {epoch+1}: Training loss = {tr_loss:.4f}\")\n",
    "    \n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss, \"train_acc\": tr_acc})\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        va_loss = 0\n",
    "        for Xb, yb in va_dl:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = model(Xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            va_loss += loss.item()\n",
    "        va_loss /= len(va_dl)\n",
    "        print(f\"Epoch {epoch+1}: Validation loss = {va_loss:.4f}\")\n",
    "        wandb.log({\"val_acc\": va_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79117b14-71bb-4558-83a0-6047c75bffaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
